<!doctype html>
<head>
<title>Neural Music Synthesis for Flexible Timbre Control</title>
<meta name="viewport" content="width=900, initial-scale=1.0">
<link rel="stylesheet" type="text/css" href="index.css">
</head>
<body>
<div id="content">
<h1>Neural Music Synthesis for Flexible Timbre Control<small> <a href="https://arxiv.org/abs/1811.00223" target="_blank">[arXiv]</a></small></h1>

<div id="authors">
    Jong Wook Kim<sup>1,2</sup>, Rachel Bittner<sup>2</sup>, Aparna Kumar<sup>2</sup>, Juan Pablo Bello<sup>1</sup><br>
    <small><sup>1</sup>Music and Audio Research Laboratory, New York University <sup>2</sup>Spotify</small>
</div>

<hr>

<img src="architecture.svg" style="width: 510px; float: right; margin: 20px;">
<h2>What is this?</h2>

<p>
    We created a neural music synthesis model named Mel2Mel,
    which consists of a recurrent neural network conditioned on a learned instrument embedding followed by a WaveNet vocoder.
    The network takes a note sequence as input and predicts the corresponding Mel spectrogram,
    which is then used for conditioning the WaveNet vocoder to produce music.
</p>
<p>
    This web demo provides synthesized audio samples and interactive visualizations of the learned timbre embedding space.
    For more details, please check out our <a href="https://arxiv.org/abs/1811.00223" target="_blank">paper</a>
    submitted to ICASSP 2019.
</p>

<h2 style="clear: both;">Synthesized Audio Samples</h2>

<p>
    To create the training dataset, we have downloaded 334 MIDI files crawled from <a href="http://www.piano-midi.de/">www.piano-midi.de</a> and 
    used <a href="http://www.fluidsynth.org/">FluidSynth</a> and the default SoundFont of <a href="https://musescore.org/">MuseScore</a> to synthesize the ground-truth audio.
</p>
<p>
    Below are the actual audio samples used in our perceptual experiments.
    The three columns to the right have the output of our music synthesis model corresponding to the three loss functions we used, and
    the three columns to the left are provided for reference.
    You can listen to 14 test tracks × 10 instruments × 6 configurations = 840 audio segments in total.
</p>

<iframe src="test-flacs.html" width="100%" height="568px" scrolling="no" frameborder="0" allowtransparency="true"></iframe>

<h2 style="clear: both;">Explore the timbre space</h2>
<p>
    The 2-dimensional timbre embedding space is visualized below, annotated with the 10 instrument icons.
    You can select either spectral centroids or mean energy for colorcoding the space.
    The Mel spectrogram is shown corresponding to the point where the mouse cursor is on, and you can listen to the synthesized sound by clicking on the embedding space.
</p>

<iframe src="explorer.html" width="100%" height="320px" scrolling="no" frameborder="0" allowtransparency="true"></iframe>

<h2>100-instrument timbre space visualization</h2>
<p>
    This is a visualization of the 100-instrument embedding space that was briefly mentioned in the paper.
    The embedding space is 10-dimensional, and a <em>t</em>-SNE in 3D was obtained for visualization.
    Each dot below represents an instrument, color-coded according to the spectral cenroid or the mean energy.
    You can drag over the cluster of dots to rotate them, and click on each dot to play a sample of the corresponding SoundFont.
</p>

<iframe src="hundred.html" width="100%" height="600px" scrolling="no" frameborder="0" allowtransparency="true"></iframe>

<h2>Try with your own MIDI files</h2>
<p style="text-align: center;">
    <a href="mel2mel.html"><img alt="TensorFlow.js demo screenshot" src="mel2mel.png" width="500"></a>
</p>
<p>
    The example above plays pre-rendered Mel spectrograms and audio. Try <a href="mel2mel.html">our TensorFlow.js demo</a>,
    where you can run the Mel2Mel model directly on browser. You can also upload your own MIDI file in this demo.
    The demo requires a WebGL-enabled browser such as desktop Chrome, and it may not run properly on mobile browsers or external monitors.
</p>


</div>
</body>
</html>
